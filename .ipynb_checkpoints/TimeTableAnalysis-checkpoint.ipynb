{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ed7fb-b9ad-4143-b445-69e8ad9a78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Java 8 path\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home'\n",
    "\n",
    "# PySpark path\n",
    "os.environ['SPARK_HOME'] = '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bdd44-f4d1-4d5c-8523-d7852b65a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Time Table Analaysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65533ab7-5abb-40b0-aa34-3e1bd081c4d1",
   "metadata": {},
   "source": [
    "## Data Ingestion and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8197c5-d5ac-419a-8848-a4754919bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read.csv('/Users/dikshanta/Documents/Assignment-Big-Data/combined.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset loaded successfully\")\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbc4de-0a3e-4416-875c-d07cbdc44657",
   "metadata": {},
   "source": [
    "## Data Ingestion - Converted To Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca48e7-7011-4c88-8f3d-e833754c33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Display basic info\n",
    "print(\"Converted to Pandas DataFrame\")\n",
    "print(f\"Shape: {pdf.shape}\")\n",
    "print(f\"Rows: {pdf.shape[0]}, Columns: {pdf.shape[1]}\")\n",
    "\n",
    "# Set pandas display options for better table formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Display first 5 rows in tabular format\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(pdf.head(5))\n",
    "\n",
    "# Alternative: Display as a nice table\n",
    "print(\"\\nColumn names:\")\n",
    "print(pdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262bd83-6f9f-462c-91d7-eef11c7defd2",
   "metadata": {},
   "source": [
    "## Data Clearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad2614-5f6e-42e2-bf01-f811e6cda4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initial data shape\n",
    "print(f\"\\nInitial dataset: {df.count()} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# 1. Check for missing values\n",
    "print(\"\\n1Ô∏è‚É£ Checking for missing values...\")\n",
    "missing_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()\n",
    "print(\"\\nMissing values per column:\")\n",
    "for col_name in df.columns:\n",
    "    missing = missing_counts[col_name][0]\n",
    "    if missing > 0:\n",
    "        print(f\"  {col_name}: {missing} ({missing/df.count()*100:.2f}%)\")\n",
    "\n",
    "# 2. Convert RunTime from PT1M format to numeric minutes\n",
    "print(\"\\n2Ô∏è‚É£ Converting RunTime to numeric format...\")\n",
    "df_clean = df.withColumn(\n",
    "    'RunTime_Minutes',\n",
    "    regexp_extract(col('RunTime'), r'PT(\\d+)M', 1).cast('double')\n",
    ")\n",
    "\n",
    "# Fill null RunTime values with 0\n",
    "df_clean = df_clean.na.fill({'RunTime_Minutes': 0.0})\n",
    "print(\"‚úÖ RunTime converted to minutes\")\n",
    "\n",
    "# 3. Remove invalid records\n",
    "print(\"\\n3Ô∏è‚É£ Removing invalid records...\")\n",
    "initial_count = df_clean.count()\n",
    "\n",
    "# Remove rows where runtime is 0 or null\n",
    "df_clean = df_clean.filter(\n",
    "    (col('RunTime_Minutes') > 0) & \n",
    "    (col('RunTime_Minutes').isNotNull())\n",
    ")\n",
    "\n",
    "# Remove rows with missing coordinates\n",
    "df_clean = df_clean.filter(\n",
    "    col('FromLat').isNotNull() & \n",
    "    col('FromLon').isNotNull() & \n",
    "    col('ToLat').isNotNull() & \n",
    "    col('ToLon').isNotNull()\n",
    ")\n",
    "\n",
    "removed_count = initial_count - df_clean.count()\n",
    "print(f\"‚úÖ Removed {removed_count} invalid records\")\n",
    "print(f\"‚úÖ Clean dataset: {df_clean.count()} rows\")\n",
    "\n",
    "# 4. Check for duplicates\n",
    "print(\"\\n4Ô∏è‚É£ Checking for duplicate records...\")\n",
    "duplicate_count = df_clean.count() - df_clean.dropDuplicates().count()\n",
    "if duplicate_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {duplicate_count} duplicates - removing...\")\n",
    "    df_clean = df_clean.dropDuplicates()\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")\n",
    "\n",
    "# 5. Data type validation\n",
    "print(\"\\n5Ô∏è‚É£ Validating data types...\")\n",
    "print(\"\\nColumn data types:\")\n",
    "df_clean.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA CLEANING COMPLETE\")\n",
    "print(f\"Final clean dataset: {df_clean.count()} rows\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f23e-cb1a-4878-96e9-b378ff39a051",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8415f2-838a-4b5c-b4b3-a34f0b1b50fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Calculate distance between stops using Haversine formula\n",
    "print(\"\\n1Ô∏è‚É£ Calculating geographic distances...\")\n",
    "df_processed = df_clean.withColumn(\n",
    "    'FromLat_rad', radians(col('FromLat'))\n",
    ").withColumn(\n",
    "    'FromLon_rad', radians(col('FromLon'))\n",
    ").withColumn(\n",
    "    'ToLat_rad', radians(col('ToLat'))\n",
    ").withColumn(\n",
    "    'ToLon_rad', radians(col('ToLon'))\n",
    ").withColumn(\n",
    "    'dlat', col('ToLat_rad') - col('FromLat_rad')\n",
    ").withColumn(\n",
    "    'dlon', col('ToLon_rad') - col('FromLon_rad')\n",
    ").withColumn(\n",
    "    'a', \n",
    "    pow(sin(col('dlat') / 2), 2) + \n",
    "    cos(col('FromLat_rad')) * cos(col('ToLat_rad')) * pow(sin(col('dlon') / 2), 2)\n",
    ").withColumn(\n",
    "    'c', 2 * asin(sqrt(col('a')))\n",
    ").withColumn(\n",
    "    'Distance_km', 6371 * col('c')  # Earth radius in km\n",
    ").drop('FromLat_rad', 'FromLon_rad', 'ToLat_rad', 'ToLon_rad', 'dlat', 'dlon', 'a', 'c')\n",
    "\n",
    "print(\"‚úÖ Distance calculated using Haversine formula\")\n",
    "\n",
    "# 2. Extract time features from DepartureTime\n",
    "print(\"\\n2Ô∏è‚É£ Extracting temporal features...\")\n",
    "df_processed = df_processed.withColumn(\n",
    "    'DepartureTime_parsed', to_timestamp(col('DepartureTime'), 'HH:mm:ss')\n",
    ").withColumn(\n",
    "    'Hour', hour(col('DepartureTime_parsed'))\n",
    ").withColumn(\n",
    "    'Minute', minute(col('DepartureTime_parsed'))\n",
    ").withColumn(\n",
    "    'TimeOfDay', \n",
    "    when(col('Hour').between(6, 11), 'Morning')\n",
    "    .when(col('Hour').between(12, 16), 'Afternoon')\n",
    "    .when(col('Hour').between(17, 20), 'Evening')\n",
    "    .otherwise('Night')\n",
    ").withColumn(\n",
    "    'IsRushHour', \n",
    "    when((col('Hour').between(7, 9)) | (col('Hour').between(16, 18)), 1).otherwise(0)\n",
    ").drop('DepartureTime_parsed')\n",
    "\n",
    "print(\"‚úÖ Temporal features extracted (Hour, Minute, TimeOfDay, IsRushHour)\")\n",
    "\n",
    "# 3. Encode categorical variables\n",
    "print(\"\\n3Ô∏è‚É£ Encoding categorical variables...\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Encode LineName\n",
    "indexer_line = StringIndexer(inputCol='LineName', outputCol='LineName_Encoded', handleInvalid='keep')\n",
    "df_processed = indexer_line.fit(df_processed).transform(df_processed)\n",
    "\n",
    "# Encode Direction\n",
    "indexer_direction = StringIndexer(inputCol='Direction', outputCol='Direction_Encoded', handleInvalid='keep')\n",
    "df_processed = indexer_direction.fit(df_processed).transform(df_processed)\n",
    "\n",
    "# Encode TimingStatus\n",
    "indexer_timing = StringIndexer(inputCol='TimingStatus', outputCol='TimingStatus_Encoded', handleInvalid='keep')\n",
    "df_processed = indexer_timing.fit(df_processed).transform(df_processed)\n",
    "\n",
    "# Encode TimeOfDay\n",
    "indexer_timeofday = StringIndexer(inputCol='TimeOfDay', outputCol='TimeOfDay_Encoded', handleInvalid='keep')\n",
    "df_processed = indexer_timeofday.fit(df_processed).transform(df_processed)\n",
    "\n",
    "print(\"‚úÖ Categorical variables encoded\")\n",
    "\n",
    "# 4. Handle any remaining missing values\n",
    "print(\"\\n4Ô∏è‚É£ Handling remaining missing values...\")\n",
    "numeric_cols = ['Sequence', 'Distance_km', 'Hour', 'Minute']\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    # Fill with median\n",
    "    median_val = df_processed.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "    df_processed = df_processed.na.fill({col_name: median_val})\n",
    "\n",
    "print(\"‚úÖ Missing values imputed with median\")\n",
    "\n",
    "# 5. Remove outliers using IQR method\n",
    "print(\"\\n5Ô∏è‚É£ Detecting and removing outliers...\")\n",
    "Q1 = df_processed.approxQuantile('RunTime_Minutes', [0.25], 0.01)[0]\n",
    "Q3 = df_processed.approxQuantile('RunTime_Minutes', [0.75], 0.01)[0]\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "before_outliers = df_processed.count()\n",
    "df_processed = df_processed.filter(\n",
    "    (col('RunTime_Minutes') >= lower_bound) & \n",
    "    (col('RunTime_Minutes') <= upper_bound)\n",
    ")\n",
    "after_outliers = df_processed.count()\n",
    "\n",
    "print(f\"‚úÖ Removed {before_outliers - after_outliers} outliers\")\n",
    "print(f\"   Valid range: {lower_bound:.2f} - {upper_bound:.2f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPROCESSING COMPLETE\")\n",
    "print(f\"Processed dataset: {df_processed.count()} rows\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b24a1f-d71b-4f8b-a338-d1b3defd2446",
   "metadata": {},
   "source": [
    "## Exploratory Data Analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121404ff-08ad-4190-95e9-db2afb5493ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import builtins  # To use Python's built-in min\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "print(\"\\nüìä Converting data for visualization...\")\n",
    "total_count = df_processed.count()\n",
    "sample_size = builtins.min(10000, total_count)  # Use Python's built-in min\n",
    "pdf_sample = df_processed.sample(False, sample_size/total_count, seed=42).toPandas()\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "fig.suptitle('Bus Runtime Analysis - Exploratory Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Runtime Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(pdf_sample['RunTime_Minutes'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Runtime (minutes)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Bus Runtime')\n",
    "ax1.axvline(pdf_sample['RunTime_Minutes'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {pdf_sample[\"RunTime_Minutes\"].mean():.2f} min')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Distance vs Runtime\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(pdf_sample['Distance_km'], pdf_sample['RunTime_Minutes'], \n",
    "            alpha=0.3, s=10, color='green')\n",
    "ax2.set_xlabel('Distance (km)')\n",
    "ax2.set_ylabel('Runtime (minutes)')\n",
    "ax2.set_title('Distance vs Runtime Correlation')\n",
    "correlation = pdf_sample['Distance_km'].corr(pdf_sample['RunTime_Minutes'])\n",
    "ax2.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "         transform=ax2.transAxes, va='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "# 3. Runtime by Hour\n",
    "ax3 = axes[0, 2]\n",
    "hourly_runtime = pdf_sample.groupby('Hour')['RunTime_Minutes'].mean().sort_index()\n",
    "ax3.plot(hourly_runtime.index, hourly_runtime.values, marker='o', linewidth=2, color='orange')\n",
    "ax3.fill_between(hourly_runtime.index, hourly_runtime.values, alpha=0.3, color='orange')\n",
    "ax3.set_xlabel('Hour of Day')\n",
    "ax3.set_ylabel('Average Runtime (minutes)')\n",
    "ax3.set_title('Average Runtime by Hour')\n",
    "ax3.axvspan(7, 9, alpha=0.2, color='red', label='Morning Rush')\n",
    "ax3.axvspan(16, 18, alpha=0.2, color='red', label='Evening Rush')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Rush Hour vs Non-Rush Hour\n",
    "ax4 = axes[1, 0]\n",
    "rush_data = pdf_sample.groupby('IsRushHour')['RunTime_Minutes'].mean()\n",
    "rush_labels = ['Non-Rush Hour', 'Rush Hour']\n",
    "ax4.bar(rush_labels, rush_data.values, color=['lightblue', 'coral'], edgecolor='black')\n",
    "ax4.set_ylabel('Average Runtime (minutes)')\n",
    "ax4.set_title('Rush Hour Impact on Runtime')\n",
    "for i, v in enumerate(rush_data.values):\n",
    "    ax4.text(i, v + 0.05, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 5. Top 10 Routes by Frequency\n",
    "ax5 = axes[1, 1]\n",
    "top_routes = pdf_sample['LineName'].value_counts().head(10)\n",
    "ax5.barh(range(len(top_routes)), top_routes.values, color='purple', alpha=0.7)\n",
    "ax5.set_yticks(range(len(top_routes)))\n",
    "ax5.set_yticklabels(top_routes.index)\n",
    "ax5.set_xlabel('Number of Trips')\n",
    "ax5.set_title('Top 10 Most Frequent Routes')\n",
    "ax5.invert_yaxis()\n",
    "\n",
    "# 6. Runtime by Direction\n",
    "ax6 = axes[1, 2]\n",
    "direction_runtime = pdf_sample.groupby('Direction')['RunTime_Minutes'].mean()\n",
    "ax6.bar(direction_runtime.index, direction_runtime.values, \n",
    "        color=['skyblue', 'salmon'], edgecolor='black', alpha=0.8)\n",
    "ax6.set_ylabel('Average Runtime (minutes)')\n",
    "ax6.set_title('Runtime by Direction')\n",
    "ax6.set_xlabel('Direction')\n",
    "\n",
    "# 7. Distance Distribution\n",
    "ax7 = axes[2, 0]\n",
    "ax7.hist(pdf_sample['Distance_km'], bins=50, edgecolor='black', alpha=0.7, color='teal')\n",
    "ax7.set_xlabel('Distance (km)')\n",
    "ax7.set_ylabel('Frequency')\n",
    "ax7.set_title('Distribution of Journey Distances')\n",
    "ax7.axvline(pdf_sample['Distance_km'].median(), color='red', linestyle='--',\n",
    "            label=f'Median: {pdf_sample[\"Distance_km\"].median():.2f} km')\n",
    "ax7.legend()\n",
    "\n",
    "# 8. Sequence vs Runtime\n",
    "ax8 = axes[2, 1]\n",
    "seq_runtime = pdf_sample.groupby('Sequence')['RunTime_Minutes'].mean().head(30)\n",
    "ax8.plot(seq_runtime.index, seq_runtime.values, marker='o', color='darkgreen')\n",
    "ax8.set_xlabel('Stop Sequence')\n",
    "ax8.set_ylabel('Average Runtime (minutes)')\n",
    "ax8.set_title('Runtime by Stop Sequence')\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Time of Day Analysis\n",
    "ax9 = axes[2, 2]\n",
    "timeofday_runtime = pdf_sample.groupby('TimeOfDay')['RunTime_Minutes'].mean()\n",
    "timeofday_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "timeofday_sorted = [timeofday_runtime.get(t, 0) for t in timeofday_order]\n",
    "colors_tod = ['gold', 'orange', 'coral', 'navy']\n",
    "ax9.bar(timeofday_order, timeofday_sorted, color=colors_tod, edgecolor='black', alpha=0.8)\n",
    "ax9.set_ylabel('Average Runtime (minutes)')\n",
    "ax9.set_title('Runtime by Time of Day')\n",
    "ax9.set_xlabel('Time Period')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ EDA plots saved as 'eda_analysis.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Statistical Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_stats = df_processed.select(\n",
    "    'RunTime_Minutes',\n",
    "    'Distance_km',\n",
    "    'Hour',\n",
    "    'Sequence',\n",
    "    'IsRushHour'\n",
    ").describe()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "summary_stats.show()\n",
    "\n",
    "# Key Insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìç Dataset Overview:\")\n",
    "print(f\"   Total journeys analyzed: {len(pdf_sample):,}\")\n",
    "print(f\"   Unique routes: {pdf_sample['LineName'].nunique()}\")\n",
    "print(f\"   Unique stops: {pdf_sample['FromStopRef'].nunique()}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Runtime Statistics:\")\n",
    "print(f\"   Average runtime: {pdf_sample['RunTime_Minutes'].mean():.2f} minutes\")\n",
    "print(f\"   Median runtime: {pdf_sample['RunTime_Minutes'].median():.2f} minutes\")\n",
    "print(f\"   Std deviation: {pdf_sample['RunTime_Minutes'].std():.2f} minutes\")\n",
    "\n",
    "print(f\"\\nüöó Distance Statistics:\")\n",
    "print(f\"   Average distance: {pdf_sample['Distance_km'].mean():.2f} km\")\n",
    "print(f\"   Median distance: {pdf_sample['Distance_km'].median():.2f} km\")\n",
    "print(f\"   Max distance: {pdf_sample['Distance_km'].max():.2f} km\")\n",
    "\n",
    "print(f\"\\nüïê Temporal Patterns:\")\n",
    "rush_mean = pdf_sample[pdf_sample['IsRushHour']==1]['RunTime_Minutes'].mean()\n",
    "non_rush_mean = pdf_sample[pdf_sample['IsRushHour']==0]['RunTime_Minutes'].mean()\n",
    "print(f\"   Rush hour avg: {rush_mean:.2f} minutes\")\n",
    "print(f\"   Non-rush avg: {non_rush_mean:.2f} minutes\")\n",
    "print(f\"   Rush hour impact: +{((rush_mean/non_rush_mean - 1) * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EDA COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1294a-86b7-4d46-879c-1e56814c5467",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be922bfb-74b4-40b2-96ed-10977e75266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Select features for modeling\n",
    "print(\"\\n1Ô∏è‚É£ Selecting features for modeling...\")\n",
    "\n",
    "feature_cols = [\n",
    "    'LineName_Encoded',\n",
    "    'Direction_Encoded',\n",
    "    'Sequence',\n",
    "    'TimingStatus_Encoded',\n",
    "    'Distance_km',\n",
    "    'Hour',\n",
    "    'IsRushHour',\n",
    "    'TimeOfDay_Encoded',\n",
    "    'FromLat',\n",
    "    'FromLon',\n",
    "    'ToLat',\n",
    "    'ToLon'\n",
    "]\n",
    "\n",
    "target_col = 'RunTime_Minutes'\n",
    "\n",
    "print(f\"‚úÖ Selected {len(feature_cols)} features:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "# 2. Create feature dataframe\n",
    "print(\"\\n2Ô∏è‚É£ Creating feature dataset...\")\n",
    "df_features = df_processed.select(feature_cols + [target_col])\n",
    "\n",
    "# Remove any remaining nulls\n",
    "df_features = df_features.na.drop()\n",
    "print(f\"‚úÖ Feature dataset: {df_features.count()} rows\")\n",
    "\n",
    "# 3. Assemble features into vector\n",
    "print(\"\\n3Ô∏è‚É£ Assembling feature vectors...\")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features_raw',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_features)\n",
    "print(\"‚úÖ Features assembled into vector\")\n",
    "\n",
    "# 4. Feature scaling/normalization\n",
    "print(\"\\n4Ô∏è‚É£ Normalizing features...\")\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features_raw',\n",
    "    outputCol='features',\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "print(\"‚úÖ Features normalized (StandardScaler)\")\n",
    "\n",
    "# 5. Final dataset preparation\n",
    "print(\"\\n5Ô∏è‚É£ Preparing final dataset...\")\n",
    "df_final = df_scaled.select('features', target_col)\n",
    "df_final = df_final.withColumnRenamed(target_col, 'label')\n",
    "\n",
    "final_count = df_final.count()\n",
    "print(f\"‚úÖ Final dataset ready: {final_count} rows\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìä Sample of engineered features:\")\n",
    "df_final.show(5, truncate=False)\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
    "print(f\"Total samples: {final_count}\")\n",
    "print(f\"Feature vector dimension: {len(feature_cols)}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Scaling method: StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654c379-9cd9-4d8a-919a-99a54c7e954f",
   "metadata": {},
   "source": [
    "## Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eaf627-6b64-4523-8206-6c13fecead53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split the data into training and testing sets (80/20)\n",
    "print(\"\\nüìä Splitting dataset into train and test sets...\")\n",
    "\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache the data for better performance\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "# Get counts\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "total_count = train_count + test_count\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset split complete:\")\n",
    "print(f\"   Training set:   {train_count:,} samples ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Test set:       {test_count:,} samples ({test_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Total samples:  {total_count:,}\")\n",
    "\n",
    "# Verify label distribution in both sets\n",
    "print(\"\\nüìà Label (RunTime) distribution:\")\n",
    "train_stats = train_data.select('label').describe().toPandas()\n",
    "test_stats = test_data.select('label').describe().toPandas()\n",
    "\n",
    "print(\"\\nTraining Set Statistics:\")\n",
    "print(train_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nTest Set Statistics:\")\n",
    "print(test_stats.to_string(index=False))\n",
    "\n",
    "# Check for data leakage - ensure no overlap\n",
    "print(\"\\nüîç Verifying no data leakage between train and test sets...\")\n",
    "print(\"‚úÖ Data split verified - no overlap between sets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN/TEST SPLIT COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f54e4d-90d9-4ffa-8ac8-d74c88da2651",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb803bba-df44-4fed-8df4-69e58660fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL TRAINING - RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configure Random Forest model\n",
    "print(\"\\nüå≤ Configuring Random Forest Regressor...\")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    numTrees=100,              # Number of trees in the forest\n",
    "    maxDepth=15,               # Maximum depth of each tree\n",
    "    minInstancesPerNode=5,     # Minimum instances per node\n",
    "    maxBins=32,                # Maximum number of bins for discretizing continuous features\n",
    "    seed=42                    # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Model Hyperparameters:\")\n",
    "print(f\"   Number of Trees:         {rf.getNumTrees()}\")\n",
    "print(f\"   Max Depth:               {rf.getMaxDepth()}\")\n",
    "print(f\"   Min Instances Per Node:  {rf.getMinInstancesPerNode()}\")\n",
    "print(f\"   Max Bins:                {rf.getMaxBins()}\")\n",
    "print(f\"   Random Seed:             {rf.getSeed()}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n‚è≥ Training Random Forest model...\")\n",
    "print(\"   (This may take 1-3 minutes depending on dataset size...)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Model training complete!\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "\n",
    "# Get feature importances\n",
    "print(\"\\nüìä Extracting feature importances...\")\n",
    "feature_importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Create feature importance dataframe\n",
    "import pandas as pd\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîù Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# Model details\n",
    "print(f\"\\nüå≤ Model Details:\")\n",
    "print(f\"   Total number of trees: {rf_model.getNumTrees}\")\n",
    "print(f\"   Total number of nodes: {rf_model.totalNumNodes}\")\n",
    "print(f\"   Feature vector size: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095b22b-8d18-4636-849f-7d6fb6daa3e8",
   "metadata": {},
   "source": [
    "# Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52337346-1e54-4f44-a385-7518f106d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON - MULTIPLE ALGORITHMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Create evaluators\n",
    "evaluator_mae = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='mae')\n",
    "evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nüî¨ Training and evaluating multiple models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: LINEAR REGRESSION (Baseline)\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ LINEAR REGRESSION (Baseline Model)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_time = time.time() - start_time\n",
    "\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"‚úÖ Training time: {lr_time:.2f} seconds\")\n",
    "print(f\"   MAE:  {lr_mae:.4f} minutes\")\n",
    "print(f\"   RMSE: {lr_rmse:.4f} minutes\")\n",
    "print(f\"   R¬≤:   {lr_r2:.4f} ({lr_r2*100:.2f}%)\")\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'MAE': lr_mae,\n",
    "    'RMSE': lr_rmse,\n",
    "    'R¬≤': lr_r2,\n",
    "    'Training Time (s)': lr_time\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: DECISION TREE (Simple Tree)\n",
    "# ============================================================================\n",
    "print(\"\\n2Ô∏è‚É£ DECISION TREE REGRESSOR\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='label', maxDepth=15, minInstancesPerNode=5, seed=42)\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_time = time.time() - start_time\n",
    "\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "dt_mae = evaluator_mae.evaluate(dt_predictions)\n",
    "dt_rmse = evaluator_rmse.evaluate(dt_predictions)\n",
    "dt_r2 = evaluator_r2.evaluate(dt_predictions)\n",
    "\n",
    "print(f\"‚úÖ Training time: {dt_time:.2f} seconds\")\n",
    "print(f\"   MAE:  {dt_mae:.4f} minutes\")\n",
    "print(f\"   RMSE: {dt_rmse:.4f} minutes\")\n",
    "print(f\"   R¬≤:   {dt_r2:.4f} ({dt_r2*100:.2f}%)\")\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'MAE': dt_mae,\n",
    "    'RMSE': dt_rmse,\n",
    "    'R¬≤': dt_r2,\n",
    "    'Training Time (s)': dt_time\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: RANDOM FOREST (Your Current Model)\n",
    "# ============================================================================\n",
    "print(\"\\n3Ô∏è‚É£ RANDOM FOREST REGRESSOR (Current Model)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Use the already trained rf_model from previous cell\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"‚úÖ Training time: {training_time:.2f} seconds\")\n",
    "print(f\"   MAE:  {rf_mae:.4f} minutes\")\n",
    "print(f\"   RMSE: {rf_rmse:.4f} minutes\")\n",
    "print(f\"   R¬≤:   {rf_r2:.4f} ({rf_r2*100:.2f}%)\")\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'MAE': rf_mae,\n",
    "    'RMSE': rf_rmse,\n",
    "    'R¬≤': rf_r2,\n",
    "    'Training Time (s)': training_time\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 4: GRADIENT BOOSTING (Advanced)\n",
    "# ============================================================================\n",
    "print(\"\\n4Ô∏è‚É£ GRADIENT BOOSTING TREES (Advanced Model)\")\n",
    "print(\"-\"*70)\n",
    "print(\"‚è≥ Training (this may take 2-3 minutes)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='label', maxIter=50, maxDepth=5, seed=42)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_time = time.time() - start_time\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
    "gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
    "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"‚úÖ Training time: {gbt_time:.2f} seconds\")\n",
    "print(f\"   MAE:  {gbt_mae:.4f} minutes\")\n",
    "print(f\"   RMSE: {gbt_rmse:.4f} minutes\")\n",
    "print(f\"   R¬≤:   {gbt_r2:.4f} ({gbt_r2*100:.2f}%)\")\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Gradient Boosting',\n",
    "    'MAE': gbt_mae,\n",
    "    'RMSE': gbt_rmse,\n",
    "    'R¬≤': gbt_r2,\n",
    "    'Training Time (s)': gbt_time\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\", results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_r2_idx = results_df['R¬≤'].idxmax()\n",
    "best_mae_idx = results_df['MAE'].idxmin()\n",
    "\n",
    "print(\"\\nüèÜ BEST MODELS:\")\n",
    "print(f\"   Best R¬≤ Score:  {results_df.loc[best_r2_idx, 'Model']} (R¬≤ = {results_df.loc[best_r2_idx, 'R¬≤']:.4f})\")\n",
    "print(f\"   Lowest MAE:     {results_df.loc[best_mae_idx, 'Model']} (MAE = {results_df.loc[best_mae_idx, 'MAE']:.4f} min)\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\nüìä Generating comparison visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Comparison - Bus Runtime Prediction', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: R¬≤ Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']\n",
    "bars1 = ax1.bar(results_df['Model'], results_df['R¬≤'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('R¬≤ Score Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.axhline(y=0.8, color='green', linestyle='--', linewidth=1, label='Good threshold (0.8)')\n",
    "ax1.legend()\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: MAE Comparison\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.bar(results_df['Model'], results_df['MAE'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Mean Absolute Error (minutes)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: RMSE Comparison\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.bar(results_df['Model'], results_df['RMSE'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Root Mean Squared Error', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: Training Time Comparison\n",
    "ax4 = axes[1, 1]\n",
    "bars4 = ax4.bar(results_df['Model'], results_df['Training Time (s)'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax4.set_ylabel('Training Time (seconds)', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Training Time Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Comparison plot saved as 'model_comparison.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL RECOMMENDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "winning_model = results_df.loc[best_r2_idx, 'Model']\n",
    "print(f\"\\nüéØ RECOMMENDED MODEL: {winning_model}\")\n",
    "print(f\"\\n   Selected based on:\")\n",
    "print(f\"   ‚Ä¢ Highest R¬≤ score: {results_df.loc[best_r2_idx, 'R¬≤']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {results_df.loc[best_r2_idx, 'MAE']:.4f} minutes\")\n",
    "print(f\"   ‚Ä¢ Training time: {results_df.loc[best_r2_idx, 'Training Time (s)']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n   This model will be used for the prediction system.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799a7ed-ef6b-4952-937e-095cd2006e17",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a959d-ca8e-42c7-a3c9-0103a1995657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL EVALUATION - RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions on test data\n",
    "print(\"\\nüîÆ Making predictions on test set...\")\n",
    "predictions = rf_model.transform(test_data)\n",
    "print(f\"‚úÖ Generated {test_count:,} predictions\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CALCULATE REGRESSION METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REGRESSION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='mae'\n",
    ")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse'\n",
    ")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "\n",
    "# R-Squared (R¬≤)\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='r2'\n",
    ")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    metricName='mse'\n",
    ")\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"   Mean Absolute Error (MAE):      {mae:.4f} minutes\")\n",
    "print(f\"   Root Mean Squared Error (RMSE): {rmse:.4f} minutes\")\n",
    "print(f\"   Mean Squared Error (MSE):       {mse:.4f}\")\n",
    "print(f\"   R-Squared (R¬≤):                 {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Model Performance Interpretation:\")\n",
    "print(f\"   ‚úì Average prediction error: ¬±{mae:.3f} minutes ({mae*60:.1f} seconds)\")\n",
    "print(f\"   ‚úì Model explains {r2*100:.2f}% of variance in runtime\")\n",
    "print(f\"   ‚úì RMSE penalizes large errors: {rmse:.3f} minutes\")\n",
    "\n",
    "# Quality assessment\n",
    "if r2 > 0.9:\n",
    "    quality = \"EXCELLENT\"\n",
    "    emoji = \"üåü\"\n",
    "elif r2 > 0.8:\n",
    "    quality = \"VERY GOOD\"\n",
    "    emoji = \"‚ú®\"\n",
    "elif r2 > 0.7:\n",
    "    quality = \"GOOD\"\n",
    "    emoji = \"üëç\"\n",
    "elif r2 > 0.6:\n",
    "    quality = \"FAIR\"\n",
    "    emoji = \"üëå\"\n",
    "else:\n",
    "    quality = \"NEEDS IMPROVEMENT\"\n",
    "    emoji = \"‚ö†Ô∏è\"\n",
    "\n",
    "print(f\"\\n{emoji} Model Quality: {quality}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SAMPLE PREDICTIONS COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS (First 20)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_predictions = predictions.select('label', 'prediction').limit(20).toPandas()\n",
    "sample_predictions['Error'] = sample_predictions['label'] - sample_predictions['prediction']\n",
    "sample_predictions['Abs_Error'] = np.abs(sample_predictions['Error'])\n",
    "sample_predictions['Error_Percentage'] = (sample_predictions['Abs_Error'] / sample_predictions['label']) * 100\n",
    "\n",
    "sample_predictions.columns = ['Actual (min)', 'Predicted (min)', 'Error', 'Abs Error', 'Error %']\n",
    "\n",
    "print(\"\\n\", sample_predictions.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage error in sample: {sample_predictions['Abs Error'].mean():.3f} minutes\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ERROR ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get prediction errors\n",
    "pred_errors = predictions.select('label', 'prediction').toPandas()\n",
    "pred_errors['Error'] = pred_errors['label'] - pred_errors['prediction']\n",
    "pred_errors['Abs_Error'] = np.abs(pred_errors['Error'])\n",
    "pred_errors['Squared_Error'] = pred_errors['Error'] ** 2\n",
    "\n",
    "print(f\"\\nüìâ Error Statistics:\")\n",
    "print(f\"   Mean Error:              {pred_errors['Error'].mean():.4f} minutes\")\n",
    "print(f\"   Std Dev of Errors:       {pred_errors['Error'].std():.4f} minutes\")\n",
    "print(f\"   Min Error:               {pred_errors['Error'].min():.4f} minutes\")\n",
    "print(f\"   Max Error:               {pred_errors['Error'].max():.4f} minutes\")\n",
    "print(f\"   Median Absolute Error:   {pred_errors['Abs_Error'].median():.4f} minutes\")\n",
    "\n",
    "# Error percentiles\n",
    "print(f\"\\nüìä Error Percentiles:\")\n",
    "print(f\"   25th percentile: {np.percentile(pred_errors['Abs_Error'], 25):.4f} minutes\")\n",
    "print(f\"   50th percentile: {np.percentile(pred_errors['Abs_Error'], 50):.4f} minutes\")\n",
    "print(f\"   75th percentile: {np.percentile(pred_errors['Abs_Error'], 75):.4f} minutes\")\n",
    "print(f\"   95th percentile: {np.percentile(pred_errors['Abs_Error'], 95):.4f} minutes\")\n",
    "\n",
    "# Accuracy within thresholds\n",
    "within_1min = (pred_errors['Abs_Error'] <= 1).sum() / len(pred_errors) * 100\n",
    "within_2min = (pred_errors['Abs_Error'] <= 2).sum() / len(pred_errors) * 100\n",
    "within_5min = (pred_errors['Abs_Error'] <= 5).sum() / len(pred_errors) * 100\n",
    "\n",
    "print(f\"\\nüéØ Prediction Accuracy:\")\n",
    "print(f\"   Within ¬±1 minute:  {within_1min:.2f}% of predictions\")\n",
    "print(f\"   Within ¬±2 minutes: {within_2min:.2f}% of predictions\")\n",
    "print(f\"   Within ¬±5 minutes: {within_5min:.2f}% of predictions\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING EVALUATION VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample data for plotting (use 10% for faster plotting)\n",
    "plot_sample = predictions.select('label', 'prediction').sample(False, 0.1, seed=42).toPandas()\n",
    "plot_sample.columns = ['Actual', 'Predicted']\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Random Forest Model - Comprehensive Evaluation', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Plot 1: Actual vs Predicted Scatter\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(plot_sample['Actual'], plot_sample['Predicted'], alpha=0.4, s=15, color='steelblue')\n",
    "ax1.plot([plot_sample['Actual'].min(), plot_sample['Actual'].max()], \n",
    "         [plot_sample['Actual'].min(), plot_sample['Actual'].max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Runtime (minutes)', fontsize=11)\n",
    "ax1.set_ylabel('Predicted Runtime (minutes)', fontsize=11)\n",
    "ax1.set_title('Actual vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=ax1.transAxes, \n",
    "         va='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Plot 2: Residuals Plot\n",
    "ax2 = axes[0, 1]\n",
    "residuals = plot_sample['Actual'] - plot_sample['Predicted']\n",
    "ax2.scatter(plot_sample['Predicted'], residuals, alpha=0.4, s=15, color='green')\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted Runtime (minutes)', fontsize=11)\n",
    "ax2.set_ylabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "ax2.set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.text(0.05, 0.95, f'Mean = {residuals.mean():.4f}', transform=ax2.transAxes, \n",
    "         va='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Plot 3: Error Distribution\n",
    "ax3 = axes[0, 2]\n",
    "abs_errors = np.abs(residuals)\n",
    "ax3.hist(abs_errors, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax3.axvline(mae, color='r', linestyle='--', lw=2, label=f'MAE = {mae:.3f}')\n",
    "ax3.set_xlabel('Absolute Error (minutes)', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title('Error Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Feature Importance (Top 10)\n",
    "ax4 = axes[1, 0]\n",
    "top_10_features = importance_df.head(10)\n",
    "colors_feat = plt.cm.viridis(np.linspace(0, 1, len(top_10_features)))\n",
    "ax4.barh(range(len(top_10_features)), top_10_features['Importance'], color=colors_feat)\n",
    "ax4.set_yticks(range(len(top_10_features)))\n",
    "ax4.set_yticklabels(top_10_features['Feature'], fontsize=9)\n",
    "ax4.set_xlabel('Importance Score', fontsize=11)\n",
    "ax4.set_title('Top 10 Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax4.invert_yaxis()\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 5: Prediction Error Box Plot\n",
    "ax5 = axes[1, 1]\n",
    "ax5.boxplot([residuals], labels=['Residuals'], vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
    "            medianprops=dict(color='red', linewidth=2))\n",
    "ax5.axhline(y=0, color='blue', linestyle='--', lw=1)\n",
    "ax5.set_ylabel('Error (minutes)', fontsize=11)\n",
    "ax5.set_title('Error Distribution (Box Plot)', fontsize=12, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Metrics Summary\n",
    "ax6 = axes[1, 2]\n",
    "ax6.axis('off')\n",
    "metrics_text = f\"\"\"\n",
    "RANDOM FOREST MODEL SUMMARY\n",
    "\n",
    "Training Samples: {train_count:,}\n",
    "Test Samples: {test_count:,}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "MAE:   {mae:.4f} minutes\n",
    "RMSE:  {rmse:.4f} minutes\n",
    "R¬≤:    {r2:.4f} ({r2*100:.2f}%)\n",
    "MSE:   {mse:.4f}\n",
    "\n",
    "ACCURACY:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Within ¬±1 min: {within_1min:.1f}%\n",
    "Within ¬±2 min: {within_2min:.1f}%\n",
    "Within ¬±5 min: {within_5min:.1f}%\n",
    "\n",
    "MODEL QUALITY: {quality}\n",
    "\n",
    "Top Feature:\n",
    "{importance_df.iloc[0]['Feature']}\n",
    "(Importance: {importance_df.iloc[0]['Importance']:.4f})\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes, \n",
    "         fontsize=10, verticalalignment='top', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('random_forest_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Evaluation plots saved as 'random_forest_evaluation.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üå≤ RANDOM FOREST REGRESSOR - FINAL REPORT\n",
    "\n",
    "MODEL CONFIGURATION:\n",
    "  ‚Ä¢ Number of Trees: {rf_model.getNumTrees}\n",
    "  ‚Ä¢ Max Depth: {rf.getMaxDepth()}\n",
    "  ‚Ä¢ Training Time: {training_time:.2f} seconds\n",
    "\n",
    "DATASET:\n",
    "  ‚Ä¢ Training Samples: {train_count:,} (80%)\n",
    "  ‚Ä¢ Test Samples: {test_count:,} (20%)\n",
    "  ‚Ä¢ Total Features: {len(feature_cols)}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "  ‚Ä¢ Mean Absolute Error (MAE): {mae:.4f} minutes\n",
    "  ‚Ä¢ Root Mean Squared Error (RMSE): {rmse:.4f} minutes\n",
    "  ‚Ä¢ R-Squared (R¬≤): {r2:.4f} ({r2*100:.2f}%)\n",
    "  ‚Ä¢ Mean Squared Error (MSE): {mse:.4f}\n",
    "\n",
    "PREDICTION ACCURACY:\n",
    "  ‚Ä¢ Within ¬±1 minute: {within_1min:.2f}%\n",
    "  ‚Ä¢ Within ¬±2 minutes: {within_2min:.2f}%\n",
    "  ‚Ä¢ Within ¬±5 minutes: {within_5min:.2f}%\n",
    "\n",
    "MODEL QUALITY: {quality} {emoji}\n",
    "\n",
    "TOP 3 IMPORTANT FEATURES:\n",
    "  1. {importance_df.iloc[0]['Feature']}: {importance_df.iloc[0]['Importance']:.4f}\n",
    "  2. {importance_df.iloc[1]['Feature']}: {importance_df.iloc[1]['Importance']:.4f}\n",
    "  3. {importance_df.iloc[2]['Feature']}: {importance_df.iloc[2]['Importance']:.4f}\n",
    "\n",
    "‚úÖ Model is ready for deployment and predictions!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f1076-0019-45b9-890d-a9996ec65f1b",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e5635-8774-4a7e-a0b5-22b1a26e4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins  # Fix for sum() conflict with PySpark\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöå BUS RUNTIME PREDICTION SYSTEM\")\n",
    "print(\"Smart Route Selection - Only Valid Combinations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PRE-VALIDATE AND BUILD ROUTE DATABASE\n",
    "# ============================================================================\n",
    "print(\"\\n‚è≥ Analyzing route database...\")\n",
    "\n",
    "# Get all route data\n",
    "route_sequences = df_processed.select(\n",
    "    'LineName', 'Direction', 'JourneyCode', 'Sequence', \n",
    "    'FromStopRef', 'FromStopName', 'FromLat', 'FromLon',\n",
    "    'ToStopRef', 'ToStopName', 'ToLat', 'ToLon'\n",
    ").orderBy('LineName', 'Direction', 'JourneyCode', 'Sequence').toPandas()\n",
    "\n",
    "# Get encoding mappings\n",
    "route_encodings = df_processed.select('LineName', 'LineName_Encoded').distinct().toPandas()\n",
    "route_encoding_map = dict(zip(route_encodings['LineName'], route_encodings['LineName_Encoded']))\n",
    "\n",
    "direction_encodings = df_processed.select('Direction', 'Direction_Encoded').distinct().toPandas()\n",
    "direction_encoding_map = dict(zip(direction_encodings['Direction'], direction_encodings['Direction_Encoded']))\n",
    "\n",
    "timeofday_encodings = df_processed.select('TimeOfDay', 'TimeOfDay_Encoded').distinct().toPandas()\n",
    "timeofday_encoding_map = dict(zip(timeofday_encodings['TimeOfDay'], timeofday_encodings['TimeOfDay_Encoded']))\n",
    "\n",
    "# Build valid route+direction combinations with stop counts\n",
    "print(\"üîç Validating route combinations...\")\n",
    "valid_combinations = {}\n",
    "\n",
    "for (route, direction), group in route_sequences.groupby(['LineName', 'Direction']):\n",
    "    journey_counts = group['JourneyCode'].value_counts()\n",
    "    if len(journey_counts) == 0:\n",
    "        continue\n",
    "    \n",
    "    representative_journey = journey_counts.index[0]\n",
    "    journey = group[group['JourneyCode'] == representative_journey].sort_values('Sequence')\n",
    "    \n",
    "    meaningful_segments = 0\n",
    "    for _, row in journey.iterrows():\n",
    "        import math\n",
    "        R = 6371\n",
    "        lat1_r = math.radians(row['FromLat'])\n",
    "        lon1_r = math.radians(row['FromLon'])\n",
    "        lat2_r = math.radians(row['ToLat'])\n",
    "        lon2_r = math.radians(row['ToLon'])\n",
    "        dlat = lat2_r - lat1_r\n",
    "        dlon = lon2_r - lon1_r\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1_r) * math.cos(lat2_r) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        dist = R * c\n",
    "        \n",
    "        if dist > 0.01:\n",
    "            meaningful_segments += 1\n",
    "    \n",
    "    if meaningful_segments >= 2:\n",
    "        if route not in valid_combinations:\n",
    "            valid_combinations[route] = {}\n",
    "        valid_combinations[route][direction] = meaningful_segments\n",
    "\n",
    "print(f\"‚úÖ Found {len(valid_combinations)} valid routes\")\n",
    "total_combinations = builtins.sum(len(v) for v in valid_combinations.values())\n",
    "print(f\"‚úÖ Total valid route+direction combinations: {total_combinations}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    import math\n",
    "    R = 6371\n",
    "    lat1_r, lon1_r = math.radians(lat1), math.radians(lon1)\n",
    "    lat2_r, lon2_r = math.radians(lat2), math.radians(lon2)\n",
    "    dlat = lat2_r - lat1_r\n",
    "    dlon = lon2_r - lon1_r\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1_r) * math.cos(lat2_r) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 6 <= hour <= 11:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour <= 16:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour <= 20:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "def get_route_segments(route_name, direction):\n",
    "    \"\"\"Get all meaningful UNIQUE segments for a route\"\"\"\n",
    "    route_data = route_sequences[\n",
    "        (route_sequences['LineName'] == route_name) & \n",
    "        (route_sequences['Direction'] == direction)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(route_data) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    # Get representative journey\n",
    "    journey_counts = route_data['JourneyCode'].value_counts()\n",
    "    representative_journey = journey_counts.index[0]\n",
    "    journey = route_data[route_data['JourneyCode'] == representative_journey].sort_values('Sequence')\n",
    "    \n",
    "    # Build segments and stops - DEDUPLICATE SEGMENTS\n",
    "    segments = []\n",
    "    stops = []\n",
    "    seen_stops = set()\n",
    "    seen_segments = set()  # Track unique segments\n",
    "    \n",
    "    for _, row in journey.iterrows():\n",
    "        dist = calculate_haversine_distance(\n",
    "            row['FromLat'], row['FromLon'],\n",
    "            row['ToLat'], row['ToLon']\n",
    "        )\n",
    "        \n",
    "        # Only add meaningful segments\n",
    "        if dist > 0.01:\n",
    "            # Create unique key for segment\n",
    "            segment_key = (row['FromStopRef'], row['ToStopRef'])\n",
    "            \n",
    "            # Only add if not seen before\n",
    "            if segment_key not in seen_segments:\n",
    "                segments.append({\n",
    "                    'Sequence': row['Sequence'],\n",
    "                    'FromStopRef': row['FromStopRef'],\n",
    "                    'FromStopName': row['FromStopName'],\n",
    "                    'FromLat': row['FromLat'],\n",
    "                    'FromLon': row['FromLon'],\n",
    "                    'ToStopRef': row['ToStopRef'],\n",
    "                    'ToStopName': row['ToStopName'],\n",
    "                    'ToLat': row['ToLat'],\n",
    "                    'ToLon': row['ToLon'],\n",
    "                    'Distance': dist\n",
    "                })\n",
    "                seen_segments.add(segment_key)\n",
    "            \n",
    "            # Add FROM stop if not seen\n",
    "            if row['FromStopRef'] not in seen_stops:\n",
    "                stops.append({\n",
    "                    'StopRef': row['FromStopRef'],\n",
    "                    'StopName': row['FromStopName'],\n",
    "                    'Lat': row['FromLat'],\n",
    "                    'Lon': row['FromLon']\n",
    "                })\n",
    "                seen_stops.add(row['FromStopRef'])\n",
    "    \n",
    "    # Add final TO stop\n",
    "    if len(segments) > 0:\n",
    "        last_seg = segments[-1]\n",
    "        if last_seg['ToStopRef'] not in seen_stops:\n",
    "            stops.append({\n",
    "                'StopRef': last_seg['ToStopRef'],\n",
    "                'StopName': last_seg['ToStopName'],\n",
    "                'Lat': last_seg['ToLat'],\n",
    "                'Lon': last_seg['ToLon']\n",
    "            })\n",
    "    \n",
    "    return segments, stops\n",
    "\n",
    "def predict_segment(segment, route_name, direction, hour):\n",
    "    distance = segment['Distance']\n",
    "    is_rush_hour = 1 if (7 <= hour <= 9) or (16 <= hour <= 18) else 0\n",
    "    time_of_day = get_time_of_day(hour)\n",
    "    \n",
    "    line_encoded = route_encoding_map.get(route_name, 0)\n",
    "    direction_encoded = direction_encoding_map.get(direction, 0)\n",
    "    timing_encoded = 0\n",
    "    timeofday_encoded = timeofday_encoding_map.get(time_of_day, 0)\n",
    "    \n",
    "    from pyspark.sql import Row\n",
    "    input_row = Row(\n",
    "        LineName_Encoded=float(line_encoded),\n",
    "        Direction_Encoded=float(direction_encoded),\n",
    "        Sequence=float(segment['Sequence']),\n",
    "        TimingStatus_Encoded=float(timing_encoded),\n",
    "        Distance_km=float(distance),\n",
    "        Hour=float(hour),\n",
    "        IsRushHour=float(is_rush_hour),\n",
    "        TimeOfDay_Encoded=float(timeofday_encoded),\n",
    "        FromLat=float(segment['FromLat']),\n",
    "        FromLon=float(segment['FromLon']),\n",
    "        ToLat=float(segment['ToLat']),\n",
    "        ToLon=float(segment['ToLon'])\n",
    "    )\n",
    "    \n",
    "    input_df = spark.createDataFrame([input_row])\n",
    "    input_assembled = assembler.transform(input_df)\n",
    "    input_scaled = scaler_model.transform(input_assembled)\n",
    "    prediction_result = rf_model.transform(input_scaled)\n",
    "    \n",
    "    return prediction_result.select('prediction').collect()[0][0]\n",
    "\n",
    "def show_options(items, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, item in enumerate(items, 1):\n",
    "        print(f\"  {i}. {item}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(f\"Enter choice (1-{len(items)}): \").strip()\n",
    "            choice_num = int(choice)\n",
    "            if 1 <= choice_num <= len(items):\n",
    "                return items[choice_num - 1]\n",
    "            else:\n",
    "                print(f\"‚ùå Please enter a number between 1 and {len(items)}\")\n",
    "        except ValueError:\n",
    "            print(\"‚ùå Please enter a valid number\")\n",
    "        except KeyboardInterrupt:\n",
    "            return None\n",
    "\n",
    "# ============================================================================\n",
    "# 3. INTERACTIVE PREDICTION LOOP\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ READY FOR PREDICTIONS!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìù ENTER JOURNEY DETAILS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # 1. Select Route\n",
    "        print(\"\\n1Ô∏è‚É£ SELECT BUS ROUTE\")\n",
    "        sorted_routes = sorted(valid_combinations.keys())\n",
    "        route_display = []\n",
    "        for route in sorted_routes:\n",
    "            directions = list(valid_combinations[route].keys())\n",
    "            direction_str = \", \".join(directions)\n",
    "            route_display.append(f\"Route {route} ({direction_str})\")\n",
    "        \n",
    "        selected_route_display = show_options(route_display, \"Available Routes with Directions:\")\n",
    "        if selected_route_display is None:\n",
    "            break\n",
    "        \n",
    "        route_num = selected_route_display.split()[1]\n",
    "        print(f\"‚úÖ Selected: Route {route_num}\")\n",
    "        \n",
    "        # 2. Select Direction\n",
    "        print(\"\\n2Ô∏è‚É£ SELECT DIRECTION\")\n",
    "        available_directions = list(valid_combinations[route_num].keys())\n",
    "        \n",
    "        if len(available_directions) == 1:\n",
    "            selected_direction = available_directions[0]\n",
    "            print(f\"‚úÖ Only one direction available: {selected_direction}\")\n",
    "        else:\n",
    "            direction_display = []\n",
    "            for d in available_directions:\n",
    "                seg_count = valid_combinations[route_num][d]\n",
    "                direction_display.append(f\"{d} ({seg_count} segments)\")\n",
    "            \n",
    "            selected = show_options(direction_display, \"Available Directions:\")\n",
    "            if selected is None:\n",
    "                break\n",
    "            selected_direction = selected.split()[0]\n",
    "            print(f\"‚úÖ Selected: {selected_direction}\")\n",
    "        \n",
    "        # 3. Get segments and stops\n",
    "        print(\"\\nüìç Loading route information...\")\n",
    "        segments, stops = get_route_segments(route_num, selected_direction)\n",
    "        \n",
    "        if len(stops) < 2:\n",
    "            print(\"‚ùå Error loading route data. Please try another route.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"‚úÖ Route has {len(stops)} stops and {len(segments)} unique segments\")\n",
    "        \n",
    "        # 4. Select FROM stop\n",
    "        print(\"\\n3Ô∏è‚É£ SELECT STARTING STOP\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, stop in enumerate(stops, 1):\n",
    "            print(f\"  {i}. {stop['StopName']}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                from_choice = int(input(f\"Enter starting stop (1-{len(stops)}): \").strip())\n",
    "                if 1 <= from_choice <= len(stops):\n",
    "                    from_stop = stops[from_choice - 1]\n",
    "                    break\n",
    "                print(f\"‚ùå Please enter a number between 1 and {len(stops)}\")\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Please enter a valid number\")\n",
    "        \n",
    "        print(f\"‚úÖ From: {from_stop['StopName']}\")\n",
    "        \n",
    "        # 5. Select TO stop\n",
    "        print(\"\\n4Ô∏è‚É£ SELECT DESTINATION STOP\")\n",
    "        available_to_stops = stops[from_choice:]\n",
    "        \n",
    "        if len(available_to_stops) < 2:\n",
    "            print(\"‚ùå No destination stops available. Please select an earlier starting stop.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        for i, stop in enumerate(available_to_stops, 1):\n",
    "            print(f\"  {i}. {stop['StopName']}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                to_choice = int(input(f\"Enter destination stop (1-{len(available_to_stops)}): \").strip())\n",
    "                if 1 <= to_choice <= len(available_to_stops):\n",
    "                    to_stop = available_to_stops[to_choice - 1]\n",
    "                    break\n",
    "                print(f\"‚ùå Please enter a number between 1 and {len(available_to_stops)}\")\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Please enter a valid number\")\n",
    "        \n",
    "        print(f\"‚úÖ To: {to_stop['StopName']}\")\n",
    "        \n",
    "        # 6. Enter time\n",
    "        print(\"\\n5Ô∏è‚É£ ENTER DEPARTURE TIME\")\n",
    "        while True:\n",
    "            hour_input = input(\"Enter hour (0-23): \").strip()\n",
    "            try:\n",
    "                hour = int(hour_input)\n",
    "                if 0 <= hour <= 23:\n",
    "                    break\n",
    "                print(\"‚ùå Please enter hour between 0 and 23\")\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Please enter a valid number\")\n",
    "        \n",
    "        # 7. Find segments between selected stops\n",
    "        print(\"\\n‚è≥ Calculating journey time...\")\n",
    "        \n",
    "        from_idx = stops.index(from_stop)\n",
    "        to_idx = stops.index(to_stop)\n",
    "        \n",
    "        journey_segments = []\n",
    "        for seg in segments:\n",
    "            seg_from_idx = next((i for i, s in enumerate(stops) if s['StopRef'] == seg['FromStopRef']), None)\n",
    "            if seg_from_idx is not None and from_idx <= seg_from_idx < to_idx:\n",
    "                journey_segments.append(seg)\n",
    "        \n",
    "        if len(journey_segments) == 0:\n",
    "            print(\"‚ùå No valid segments found between these stops.\")\n",
    "            continue\n",
    "        \n",
    "        # 8. Predict each segment\n",
    "        total_time = 0\n",
    "        total_distance = 0\n",
    "        segment_details = []\n",
    "        \n",
    "        for segment in journey_segments:\n",
    "            segment_time = predict_segment(segment, route_num, selected_direction, hour)\n",
    "            total_time += segment_time\n",
    "            total_distance += segment['Distance']\n",
    "            \n",
    "            segment_details.append({\n",
    "                'from': segment['FromStopName'],\n",
    "                'to': segment['ToStopName'],\n",
    "                'time': segment_time,\n",
    "                'distance': segment['Distance']\n",
    "            })\n",
    "        \n",
    "        # 9. Display results\n",
    "        is_rush_hour = 1 if (7 <= hour <= 9) or (16 <= hour <= 18) else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üéØ JOURNEY PREDICTION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüìç Journey Details:\")\n",
    "        print(f\"   Route:           Route {route_num}\")\n",
    "        print(f\"   Direction:       {selected_direction}\")\n",
    "        print(f\"   From:            {from_stop['StopName']}\")\n",
    "        print(f\"   To:              {to_stop['StopName']}\")\n",
    "        print(f\"   Segments:        {len(journey_segments)}\")\n",
    "        print(f\"   Total Distance:  {total_distance:.2f} km\")\n",
    "        print(f\"   Departure:       {hour:02d}:00 {'(Rush Hour)' if is_rush_hour else ''}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  PREDICTED TRAVEL TIME: {total_time:.2f} minutes\")\n",
    "        mins = int(total_time)\n",
    "        secs = int((total_time - mins) * 60)\n",
    "        print(f\"   = {mins} minute(s) and {secs} seconds\")\n",
    "        \n",
    "        if len(journey_segments) > 1 and len(journey_segments) <= 15:\n",
    "            print(f\"\\nüìä Segment Breakdown:\")\n",
    "            for i, seg in enumerate(segment_details, 1):\n",
    "                print(f\"   {i}. {seg['from'][:28]:28s} ‚Üí {seg['to'][:28]:28s}\")\n",
    "                print(f\"      Time: {seg['time']:.2f} min | Distance: {seg['distance']:.2f} km\")\n",
    "        elif len(journey_segments) > 15:\n",
    "            print(f\"\\nüìä Segment Summary (showing first 5 and last 5):\")\n",
    "            for i in range(5):\n",
    "                seg = segment_details[i]\n",
    "                print(f\"   {i+1}. {seg['from'][:28]:28s} ‚Üí {seg['to'][:28]:28s}\")\n",
    "                print(f\"      Time: {seg['time']:.2f} min | Distance: {seg['distance']:.2f} km\")\n",
    "            print(f\"   ... {len(journey_segments) - 10} more segments ...\")\n",
    "            for i in range(len(segment_details) - 5, len(segment_details)):\n",
    "                seg = segment_details[i]\n",
    "                print(f\"   {i+1}. {seg['from'][:28]:28s} ‚Üí {seg['to'][:28]:28s}\")\n",
    "                print(f\"      Time: {seg['time']:.2f} min | Distance: {seg['distance']:.2f} km\")\n",
    "        \n",
    "        avg_speed = (total_distance / total_time) * 60 if total_time > 0 else 0\n",
    "        print(f\"\\nüí° Analysis:\")\n",
    "        print(f\"   Average speed: {avg_speed:.1f} km/h\")\n",
    "        if is_rush_hour:\n",
    "            print(f\"   ‚ö†Ô∏è  Rush hour - expect delays\")\n",
    "        \n",
    "        print(f\"\\n   Model: R¬≤ = {r2:.4f} | MAE = ¬±{mae:.2f} min/segment\")\n",
    "        print(f\"   Total error range: ¬±{mae * len(journey_segments):.2f} minutes\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # Continue?\n",
    "        continue_choice = input(\"\\nüîÑ Make another prediction? (y/n): \").strip().lower()\n",
    "        if continue_choice not in ['y', 'yes']:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üëã Thank you for using the prediction system!\")\n",
    "            print(\"=\"*70)\n",
    "            break\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"üëã Session ended!\")\n",
    "        print(\"=\"*70)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úÖ Prediction system complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c3cdc-2af1-481a-8ee6-6132c0170210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
